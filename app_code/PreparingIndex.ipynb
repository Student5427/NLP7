{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c813773-0e8c-4385-92ce-95deff4f9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Импорт библиотек\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085beb2a-0fd4-474c-82f2-98933414df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Загрузка\n",
    "df = pd.read_csv('../shared_data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd83d86-2ac7-4ad1-a365-57027e3d0cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# 3. Загрузка стоп-слов и инициализация стеммера\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # именно этот пакет нужен для русского языка\n",
    "\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c3b95d-db89-4093-a01e-7fa52e81d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Предобработка текста: очистка, токенизация, удаление стоп-слов и стемминг\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Приведение к нижнему регистру и удаление знаков препинания/цифр\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ\\s]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Удаление стоп-слов и стемминг\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in russian_stopwords and len(token) > 2:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            processed_tokens.append(stemmed_token)\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79886859-fabd-4e7c-88d8-f6032118cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Усовершенствованная ф-ия предобработки текста (учитывает числовые данные, слова с дефисом)\n",
    "def improved_preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Улучшенная предобработка с более интеллектуальной токенизацией\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Шаг 1: Более аккуратная очистка\n",
    "    # Сохраняем числа, дефисы в словах, email-адреса\n",
    "    text = str(text)\n",
    "    \n",
    "    # Шаг 2: Используем интеллектуальную токенизацию nltk\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    \n",
    "    # Шаг 3: Фильтрация и нормализация\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Приводим к нижнему регистру\n",
    "        token_lower = token.lower()\n",
    "        \n",
    "        # Пропускаем стоп-слова\n",
    "        if token_lower in russian_stopwords:\n",
    "            continue\n",
    "            \n",
    "        # Пропускаем короткие токены (кроме чисел и специальных случаев)\n",
    "        if len(token_lower) <= 2 and not token_lower.isdigit():\n",
    "            continue\n",
    "            \n",
    "        # Пропускаем чисто знаки препинания\n",
    "        if re.match(r'^[^\\w\\s]+$', token_lower):\n",
    "            continue\n",
    "            \n",
    "        # Стемминг для русских слов\n",
    "        if re.match(r'[а-яё]+', token_lower):\n",
    "            stemmed_token = stemmer.stem(token_lower)\n",
    "            processed_tokens.append(stemmed_token)\n",
    "        else:\n",
    "            # Для английских слов, чисел, специальных терминов оставляем как есть\n",
    "            processed_tokens.append(token_lower)\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfeba8e-a4cf-4c41-9a55-e5ace2e175f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Построение инвертированного индекса\n",
    "def build_inverted_index(df, text_columns=['title', 'text']):\n",
    "    \"\"\"\n",
    "    Построение инвертированного индекса из DataFrame\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Объединяем текст из указанных колонок\n",
    "        combined_text = ' '.join(str(row[col]) for col in text_columns if col in row and pd.notna(row[col]))\n",
    "        \n",
    "        # Предобработка текста\n",
    "        tokens = improved_preprocess_text(combined_text) #Функция заменена на improved\n",
    "        doc_lengths[idx] = len(tokens)\n",
    "        \n",
    "        # Подсчет TF (Term Frequency) для документа\n",
    "        term_freq = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            term_freq[token] += 1\n",
    "        \n",
    "        # Добавление в инвертированный индекс\n",
    "        for token, freq in term_freq.items():\n",
    "            inverted_index[token][idx] = freq\n",
    "    \n",
    "    return inverted_index, doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937d1428-5625-4fd9-b9b8-3d3b843c9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Функции для сохранения и загрузки индекса\n",
    "def save_index(inverted_index, doc_lengths, filename='../shared_data/search_index.pkl'):\n",
    "    \"\"\"\n",
    "    Сохранение индекса в файл\n",
    "    \"\"\"\n",
    "    index_data = {\n",
    "        'inverted_index': dict(inverted_index),\n",
    "        'doc_lengths': doc_lengths\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(index_data, f)\n",
    "    print(f\"Индекс сохранен в {filename}\")\n",
    "\n",
    "def load_index(filename='../shared_data/search_index.pkl'):\n",
    "    \"\"\"\n",
    "    Загрузка индекса из файла\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        print(f\"Индекс загружен из {filename}\")\n",
    "        return defaultdict(dict, index_data['inverted_index']), index_data['doc_lengths']\n",
    "    else:\n",
    "        print(f\"Файл {filename} не найден\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376e7fc0-4049-4a25-97b3-9f4e1e366b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем построение инвертированного индекса...\n",
      "Размер датасета: 21673 строк\n",
      "Колонки: ['N', 'source', 'rubric', 'title', 'text']\n",
      "\n",
      "Первые 3 заголовка:\n",
      "0: Синий богатырь...\n",
      "1: Загитова согласилась вести «Ледниковый период»...\n",
      "2: Объяснена опасность однообразного питания...\n"
     ]
    }
   ],
   "source": [
    "# 7. Построение и сохранение индекса\n",
    "print(\"Начинаем построение инвертированного индекса...\")\n",
    "\n",
    "# Проверяем структуру данных\n",
    "print(f\"Размер датасета: {len(df)} строк\")\n",
    "print(f\"Колонки: {df.columns.tolist()}\")\n",
    "print(\"\\nПервые 3 заголовка:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"{i}: {df.iloc[i]['title'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868f2176-95f8-4bda-8bd4-3dd0c6002a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим индекс\n",
    "inverted_index, doc_lengths = build_inverted_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bc63d8-d223-40f0-a63b-310c9672d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индекс сохранен в ../shared_data/search_index.pkl\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем индекс\n",
    "save_index(inverted_index, doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb0486e-f8a6-4724-b0b4-4cb8378c2d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== СТАТИСТИКА ИНДЕКСА ===\n",
      "Количество документов: 21673\n",
      "Количество уникальных терминов: 104125\n",
      "Размер индекса в памяти: 2718491 записей\n"
     ]
    }
   ],
   "source": [
    "# 8. Статистика индекса\n",
    "print(f\"\\n=== СТАТИСТИКА ИНДЕКСА ===\")\n",
    "print(f\"Количество документов: {len(df)}\")\n",
    "print(f\"Количество уникальных терминов: {len(inverted_index)}\")\n",
    "print(f\"Размер индекса в памяти: {sum(len(docs) for docs in inverted_index.values())} записей\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d14248d-3609-4c54-a873-531aeff66e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 самых частых терминов:\n",
      "  новост: встречается в 16021 документах\n",
      "  ри: встречается в 15198 документах\n",
      "  котор: встречается в 13538 документах\n",
      "  москв: встречается в 11071 документах\n",
      "  росс: встречается в 9879 документах\n",
      "  год: встречается в 9781 документах\n",
      "  эт: встречается в 9757 документах\n",
      "  такж: встречается в 8984 документах\n",
      "  дан: встречается в 8472 документах\n",
      "  ран: встречается в 7387 документах\n"
     ]
    }
   ],
   "source": [
    "# Топ-10 самых частых терминов\n",
    "term_doc_freq = {term: len(docs) for term, docs in inverted_index.items()}\n",
    "top_terms = sorted(term_doc_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f\"\\nТоп-10 самых частых терминов:\")\n",
    "for term, freq in top_terms:\n",
    "    print(f\"  {term}: встречается в {freq} документах\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19adb3-ce9e-4634-874c-e37fcf71600a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
