{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed8dc0e-b7b4-4906-ae38-6e47480f2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Импорт библиотек\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd01f31b-f603-4e76-8fb6-9ec062c4bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Загрузка данных\n",
    "df = pd.read_csv('../shared_data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e98c7d-e5b9-4625-bbd2-2057ef4b0825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализация pymorphy3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 3. Инициализация pymorphy3 и стоп-слов\n",
    "print(\"Инициализация pymorphy3...\")\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e4b4dd-ca3a-49be-9f90-76c78e521c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Функция лемматизации с кэшированием для ускорения\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = pymorphy3.MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, word):\n",
    "        if word not in self.cache:\n",
    "            parsed = self.morph.parse(word)[0]\n",
    "            self.cache[word] = parsed.normal_form\n",
    "        return self.cache[word]\n",
    "\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6775d3b2-49e5-4fc1-845f-a8abf8e48f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Функция предобработки с лемматизацией\n",
    "def preprocess_text_with_lemmatization(text):\n",
    "    \"\"\"\n",
    "    Предобработка текста с лемматизацией вместо стемминга\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Очистка текста\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ\\s]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in russian_stopwords and len(token) > 2:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            # Фильтрация слишком коротких лемм и служебных частей речи\n",
    "            if len(lemma) > 2:\n",
    "                processed_tokens.append(lemma)\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2398c4f2-90ed-40fc-b25e-ca5e58ed74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Построение инвертированного индекса с лемматизацией\n",
    "def build_lemmatized_inverted_index(df, text_columns=['title', 'text']):\n",
    "    \"\"\"\n",
    "    Построение инвертированного индекса с использованием лемматизации\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "    \n",
    "    print(\"Начинаем построение индекса с лемматизацией...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Обработано {idx}/{len(df)} документов...\")\n",
    "        \n",
    "        # Объединяем текст из указанных колонок\n",
    "        combined_text = ' '.join(str(row[col]) for col in text_columns if col in row and pd.notna(row[col]))\n",
    "        \n",
    "        # Предобработка текста с лемматизацией\n",
    "        tokens = preprocess_text_with_lemmatization(combined_text)\n",
    "        doc_lengths[idx] = len(tokens)\n",
    "        \n",
    "        # Подсчет TF (Term Frequency) для документа\n",
    "        term_freq = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            term_freq[token] += 1\n",
    "        \n",
    "        # Добавление в инвертированный индекс\n",
    "        for token, freq in term_freq.items():\n",
    "            inverted_index[token][idx] = freq\n",
    "    \n",
    "    return inverted_index, doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29030cbf-0925-441a-bce1-aaaf33a9356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Функции для сохранения и загрузки лемматизированного индекса\n",
    "def save_lemmatized_index(inverted_index, doc_lengths, filename='../shared_data/lemmatized_search_index.pkl'):\n",
    "    \"\"\"\n",
    "    Сохранение лемматизированного индекса в файл\n",
    "    \"\"\"\n",
    "    index_data = {\n",
    "        'inverted_index': dict(inverted_index),\n",
    "        'doc_lengths': doc_lengths,\n",
    "        'type': 'lemmatized'\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(index_data, f)\n",
    "    print(f\"Лемматизированный индекс сохранен в {filename}\")\n",
    "\n",
    "def load_lemmatized_index(filename='../shared_data/lemmatized_search_index.pkl'):\n",
    "    \"\"\"\n",
    "    Загрузка лемматизированного индекса из файла\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        print(f\"Лемматизированный индекс загружен из {filename}\")\n",
    "        return defaultdict(dict, index_data['inverted_index']), index_data['doc_lengths']\n",
    "    else:\n",
    "        print(f\"Файл {filename} не найден\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a0faaa-fd35-4dcb-8ea7-9f541ecd7c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПОСТРОЕНИЕ ИНДЕКСА С ЛЕММАТИЗАЦИЕЙ ===\n",
      "Размер датасета: 21673 строк\n",
      "Колонки: ['N', 'source', 'rubric', 'title', 'text']\n",
      "Начинаем построение индекса с лемматизацией...\n",
      "Обработано 0/21673 документов...\n",
      "Обработано 1000/21673 документов...\n",
      "Обработано 2000/21673 документов...\n",
      "Обработано 3000/21673 документов...\n",
      "Обработано 4000/21673 документов...\n",
      "Обработано 5000/21673 документов...\n",
      "Обработано 6000/21673 документов...\n",
      "Обработано 7000/21673 документов...\n",
      "Обработано 8000/21673 документов...\n",
      "Обработано 9000/21673 документов...\n",
      "Обработано 10000/21673 документов...\n",
      "Обработано 11000/21673 документов...\n",
      "Обработано 12000/21673 документов...\n",
      "Обработано 13000/21673 документов...\n",
      "Обработано 14000/21673 документов...\n",
      "Обработано 15000/21673 документов...\n",
      "Обработано 16000/21673 документов...\n",
      "Обработано 17000/21673 документов...\n",
      "Обработано 18000/21673 документов...\n",
      "Обработано 19000/21673 документов...\n",
      "Обработано 20000/21673 документов...\n",
      "Обработано 21000/21673 документов...\n",
      "Лемматизированный индекс сохранен в ../shared_data/lemmatized_search_index.pkl\n"
     ]
    }
   ],
   "source": [
    "# 8. Построение и сохранение лемматизированного индекса\n",
    "print(\"=== ПОСТРОЕНИЕ ИНДЕКСА С ЛЕММАТИЗАЦИЕЙ ===\")\n",
    "\n",
    "# Проверяем данные\n",
    "print(f\"Размер датасета: {len(df)} строк\")\n",
    "print(f\"Колонки: {df.columns.tolist()}\")\n",
    "\n",
    "# Строим индекс с лемматизацией\n",
    "lemmatized_index, lemmatized_doc_lengths = build_lemmatized_inverted_index(df)\n",
    "\n",
    "# Сохраняем индекс\n",
    "save_lemmatized_index(lemmatized_index, lemmatized_doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8486a4-8bbf-47f8-a110-2a788488a585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== СТАТИСТИКА ЛЕММАТИЗИРОВАННОГО ИНДЕКСА ===\n",
      "Количество документов: 21673\n",
      "Количество уникальных лемм: 85308\n"
     ]
    }
   ],
   "source": [
    "# 9. Статистика лемматизированного индекса\n",
    "print(f\"\\n=== СТАТИСТИКА ЛЕММАТИЗИРОВАННОГО ИНДЕКСА ===\")\n",
    "print(f\"Количество документов: {len(df)}\")\n",
    "print(f\"Количество уникальных лемм: {len(lemmatized_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb3bf47-7ea5-46ec-a5e9-4929a280651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 самых частых лемм:\n",
      "  новость: встречается в 16021 документах\n",
      "  риа: встречается в 15194 документах\n",
      "  который: встречается в 13537 документах\n",
      "  москва: встречается в 11071 документах\n",
      "  год: встречается в 10844 документах\n",
      "  россия: встречается в 9858 документах\n",
      "  человек: встречается в 9231 документах\n",
      "  также: встречается в 8984 документах\n",
      "  это: встречается в 8722 документах\n",
      "  страна: встречается в 7159 документах\n"
     ]
    }
   ],
   "source": [
    "# Топ-10 самых частых лемм\n",
    "lemma_doc_freq = {lemma: len(docs) for lemma, docs in lemmatized_index.items()}\n",
    "top_lemmas = sorted(lemma_doc_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f\"\\nТоп-10 самых частых лемм:\")\n",
    "for lemma, freq in top_lemmas:\n",
    "    print(f\"  {lemma}: встречается в {freq} документах\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12287e-1b48-41e9-ab7a-2467faa8b6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
